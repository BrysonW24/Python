# Hyperparameter tuning for decision tree with random search
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, make_scorer

# Gradient Boosting hyperparameter tuning

name = "Gradient Boosting"

# Define the parameter grid
param_grid = {
    'n_estimators': range(50, 150, 10),
    'max_depth': (range(1, 20)),
    'min_samples_split': range(2, 20),
    'min_samples_leaf': range(1, 20),
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_leaf_nodes': (range(1, 20)),
    'criterion': ['friedman_mse', 'mse', 'mae'],
    'loss': ['deviance', 'exponential'],
    'subsample': np.arange(0.1, 1.1, 0.1),
    'learning_rate': np.arange(0.1, 1.1, 0.1)
}

# Create a GBM Classifier with the parameter grid
gbm_clf = GradientBoostingClassifier()   # random_state=17052024

# we will use multiple scoring metrics
scoring = {"Acc": make_scorer(accuracy_score), "F1": "f1", "Recall": "recall", "Precision": "precision", "AUC": "roc_auc"}

# Instantiate the grid search model
# grid_search = GridSearchCV(estimator=gbm_clf, param_grid=param_grid, cv=3)

# Define random search
random_search = RandomizedSearchCV(estimator=gbm_clf, 
                                   param_distributions=param_grid, cv=5, n_jobs=-1,
                                    n_iter=500, scoring=scoring, verbose=1, refit="F1")

# Fit the grid search to the data
# grid_search.fit(X_train, y_train)    # this takes 45 minutes
random_search.fit(X_train, y_train)

# Get the best parameters
best_params_gbm = random_search.best_params_

print(f'Best parameters for GBM: /n {best_params_gbm}')

# Train the model with best parameters
#best_grid = grid_search.best_estimator_
best_grid = random_search.best_estimator_
best_grid.fit(X_train, y_train)

# Model Evaluation
y_pred = best_grid.predict(X_test)
y_pred_proba = best_grid.predict_proba(X_test)[:, 1]

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Gradient Boosting')
plt.legend(loc="lower right")
plt.show()

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print evaluation metrics
print("Classification Report:\n")
print(classification_report(y_test, y_pred))
print('--------------------------------')
print(f"Model: {name}")
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Precision:", metrics.precision_score(y_test, y_pred))
print("Recall:", metrics.recall_score(y_test, y_pred))
print("F1 score:", metrics.f1_score(y_test, y_pred))



# Random forest hyperparameter tuning

name = "Random Forest"

# Define the parameter grid
param_grid = {
    # 'n_estimators': range(50, 150, 10),
    # 'max_depth': (range(1, 20)),
    # 'min_samples_split': range(2, 20),
    # 'min_samples_leaf': range(1, 20),
    # 'max_features': ['auto', 'sqrt', 'log2'],
    # 'max_leaf_nodes': (range(1, 20)),
    # 'class_weight': ['balanced', 'balanced_subsample', None],
    # 'criterion': ['gini', 'entropy']
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a Random Forest Classifier with the parameter grid
rf_clf = RandomForestClassifier(random_state=17052024)

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params_rf = grid_search.best_params_

print(f'Best parameters for Random Forest: {best_params_rf}')

# Train the model with best parameters
best_grid = grid_search.best_estimator_
best_grid.fit(X_train, y_train)

# Model Evaluation
y_pred = best_grid.predict(X_test)
y_pred_proba = best_grid.predict_proba(X_test)[:, 1]

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Random Forest')
plt.legend(loc="lower right")
plt.show()

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print evaluation metrics
print("Classification Report:\n")
print(classification_report(y_test, y_pred))
print('--------------------------------')
print(f"Model: {name}")
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Precision:", metrics.precision_score(y_test, y_pred))
print("Recall:", metrics.recall_score(y_test, y_pred))
print("F1 score:", metrics.f1_score(y_test, y_pred))


# Extreme Gradient Boosting hyperparameter tuning

name = "Extreme Gradient Boosting"

# Define the parameter grid
param_grid = {
    'n_estimators': range(50, 150, 10),
    'max_depth': (range(1, 20)),
    'min_samples_split': range(2, 20),
    'min_samples_leaf': range(1, 20),
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_leaf_nodes': (range(1, 20)),
    'criterion': ['friedman_mse', 'mse', 'mae'],
    'loss': ['deviance', 'exponential'],
    'subsample': np.arange(0.1, 1.1, 0.1),
    'learning_rate': np.arange(0.1, 1.1, 0.1)
}

# Create a GBM Classifier with the parameter grid
xgbm_clf = XGBClassifier()

# Instantiate the grid search model
#grid_search = GridSearchCV(estimator=xgbm_clf, param_grid=param_grid, cv=3)
random_search = RandomizedSearchCV(estimator=xgbm_clf, 
                                   param_distributions=param_grid, cv=5, n_jobs=-1,
                                    n_iter=500, scoring=scoring, verbose=1, refit="F1")

# Fit the grid search to the data
# grid_search.fit(X_train, y_train)
random_search.fit(X_train, y_train)

# Get the best parameters
best_params_xgbm = random_search.best_params_

print(f'Best parameters for xGBM: {best_params_xgbm}')

# Train the model with best parameters
best_grid = random_search.best_estimator_
best_grid.fit(X_train, y_train)

# Model Evaluation
y_pred = best_grid.predict(X_test)
y_pred_proba = best_grid.predict_proba(X_test)[:, 1]

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Random Forest')
plt.legend(loc="lower right")
plt.show()

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print evaluation metrics
print("Classification Report:\n")
print(classification_report(y_test, y_pred))
print('--------------------------------')
print(f"Model: {name}")
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Precision:", metrics.precision_score(y_test, y_pred))
print("Recall:", metrics.recall_score(y_test, y_pred))
print("F1 score:", metrics.f1_score(y_test, y_pred))


# Decision tree hyperparameter tuning

name = "Decision Tree"

param_grid = {
    # 'max_depth': (range(1, 20)),
    # 'min_samples_split': range(2, 20),
    # 'min_samples_leaf': range(1, 20),
    # 'max_features': ['auto', 'sqrt', 'log2'],
    # 'max_leaf_nodes': (range(1, 20)),
    # 'class_weight': ['balanced', None],
    # 'criterion': ['gini', 'entropy']
    'max_depth': np.arange(5, 10, 15),
    'min_samples_split': np.arange(2, 8, 15),
    'min_samples_leaf': np.arange(1, 6, 15),
}

# Create a Decision Tree Classifier with the parameter grid
clf = DecisionTreeClassifier() #random_state=17052024

grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params_dt = grid_search.best_params_
print(f'Best parameters for Random Forest: {best_params_dt}')

# Train the model with best parameters
best_grid = grid_search.best_estimator_
best_grid.fit(X_train, y_train)

# Visualise the decision tree
plt.figure(figsize=(15,7.5))
plot_tree(best_grid, filled=True, rounded=True,
          feature_names=df_feature.columns,
          class_names=['Non-Churner','Churner'])
plt.show()

# Model Evaluation
y_pred = best_grid.predict(X_test)
y_pred_proba = best_grid.predict_proba(X_test)[:, 1]

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Decision Tree')
plt.legend(loc="lower right")
plt.show()

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print evaluation metrics
print("Classification Report:\n")
print(classification_report(y_test, y_pred))
print('--------------------------------')
print(f"Model: {name}")
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Precision:", metrics.precision_score(y_test, y_pred))
print("Recall:", metrics.recall_score(y_test, y_pred))
print("F1 score:", metrics.f1_score(y_test, y_pred))
